- PERSPECTIVES ON ARTIFICIAL INTELLIGENCE: WHERE WE ARE AND THE NEXT FRONTIER IN FINANCIAL SERVICES
[House Hearing, 116 Congress]
[From the U.S. Government Publishing Office]
PERSPECTIVES ON ARTIFICIAL INTELLIGENCE:
WHERE WE ARE AND THE NEXT
FRONTIER IN FINANCIAL SERVICES
=======================================================================
HEARING
BEFORE THE
TASK FORCE ON ARTIFICIAL INTELLIGENCE
OF THE
COMMITTEE ON FINANCIAL SERVICES
U.S. HOUSE OF REPRESENTATIVES
ONE HUNDRED SIXTEENTH CONGRESS
FIRST SESSION
__________
JUNE 26, 2019
__________
Printed for the use of the Committee on Financial Services
Serial No. 116-37
[GRAPHIC NOT AVAILABLE IN TIFF FORMAT]
__________
U.S. GOVERNMENT PUBLISHING OFFICE
39-737 PDF
WASHINGTON : 2020
--------------------------------------------------------------------------------------
HOUSE COMMITTEE ON FINANCIAL SERVICES
MAXINE WATERS, California, Chairwoman
CAROLYN B. MALONEY, New York
PATRICK McHENRY, North Carolina,
NYDIA M. VELAZQUEZ, New York
Ranking Member
BRAD SHERMAN, California
PETER T. KING, New York
GREGORY W. MEEKS, New York
FRANK D. LUCAS, Oklahoma
WM. LACY CLAY, Missouri
BILL POSEY, Florida
DAVID SCOTT, Georgia
BLAINE LUETKEMEYER, Missouri
AL GREEN, Texas
BILL HUIZENGA, Michigan
EMANUEL CLEAVER, Missouri
SEAN P. DUFFY, Wisconsin
ED PERLMUTTER, Colorado
STEVE STIVERS, Ohio
JIM A. HIMES, Connecticut
ANN WAGNER, Missouri
BILL FOSTER, Illinois
ANDY BARR, Kentucky
JOYCE BEATTY, Ohio
SCOTT TIPTON, Colorado
DENNY HECK, Washington
ROGER WILLIAMS, Texas
JUAN VARGAS, California
FRENCH HILL, Arkansas
JOSH GOTTHEIMER, New Jersey
TOM EMMER, Minnesota
VICENTE GONZALEZ, Texas
LEE M. ZELDIN, New York
AL LAWSON, Florida
BARRY LOUDERMILK, Georgia
MICHAEL SAN NICOLAS, Guam
ALEXANDER X. MOONEY, West Virginia
RASHIDA TLAIB, Michigan
WARREN DAVIDSON, Ohio
KATIE PORTER, California
TED BUDD, North Carolina
CINDY AXNE, Iowa
DAVID KUSTOFF, Tennessee
SEAN CASTEN, Illinois
TREY HOLLINGSWORTH, Indiana
AYANNA PRESSLEY, Massachusetts
ANTHONY GONZALEZ, Ohio
BEN McADAMS, Utah
JOHN ROSE, Tennessee
ALEXANDRIA OCASIO-CORTEZ, New York
BRYAN STEIL, Wisconsin
JENNIFER WEXTON, Virginia
LANCE GOODEN, Texas
STEPHEN F. LYNCH, Massachusetts
DENVER RIGGLEMAN, Virginia
TULSI GABBARD, Hawaii
ALMA ADAMS, North Carolina
MADELEINE DEAN, Pennsylvania
JESUS ``CHUY'' GARCIA, Illinois
SYLVIA GARCIA, Texas
DEAN PHILLIPS, Minnesota
Charla Ouertatani, Staff Director
TASK FORCE ON ARTIFICIAL INTELLIGENCE
BILL FOSTER, Illinois, Chairman
EMANUEL CLEAVER, Missouri
FRENCH HILL, ARKANSAS, Ranking
KATIE PORTER, California
Member
SEAN CASTEN, Illinois
BARRY LOUDERMILK, Georgia,
ALMA ADAMS, North Carolina
TED BUDD, North Carolina
SYLVIA GARCIA, Texas
TREY HOLLINGSWORTH, Indiana
DEAN PHILLIPS, Minnesota
ANTHONY GONZALEZ, Ohio
DENVER RIGGLEMAN, Virginia
C O N T E N T S
----------
Page
Hearing held on:
June 26, 2019................................................
1
Appendix:
June 26, 2019................................................
33
WITNESSES
Wednesday, June 26, 2019
Buchanan, Bonnie, Head of Department of Finance and Accounting,
Full Professor of Finance, Surrey Business School, The
University of Surrey...........................................
6
McWaters, R. Jesse, Financial Innovation Lead, World Economic
Forum..........................................................
10
Merrill, Douglas, Founder and CEO, ZestFinance...................
8
Turner-Lee, Nicol, Fellow, Center for Technology Innovation,
Brookings Institution..........................................
4
APPENDIX
Prepared statements:
Buchanan, Bonnie.............................................
34
McWaters, R. Jesse...........................................
46
Merrill, Douglas.............................................
54
Turner-Lee, Nicol............................................
109
Additional Material Submitted for the Record
Budd, Hon. Ted:
GAO report entitled, ``Insurance Markets: Benefits and
Challenges Presented by Innovative Uses of Technology,''
dated June 2019............................................
127
Hill, Hon. French:
ZestFinance article entitled, ``Clarifying Why SHAP Shouldn't
Be Used Alone''............................................
170
PERSPECTIVES ON ARTIFICIAL
INTELLIGENCE: WHERE WE ARE
AND THE NEXT FRONTIER IN
FINANCIAL SERVICES
----------
Wednesday, June 26, 2019
U.S. House of Representatives,
Task Force on Artificial Intelligence,
Committee on Financial Services,
Washington, D.C.
The task force met, pursuant to notice, at 10 a.m., in room
2128, Rayburn House Office Building, Hon. Bill Foster [chairman
of the task force] presiding.
Members present: Representatives Foster, Casten, Adams,
Garcia of Texas, Phillips; Hill, Loudermilk, Budd,
Hollingsworth, Gonzalez of Ohio, and Riggleman.
Also present: Representative Himes.
Chairman Foster. The Task Force on Artificial Intelligence
will now come to order.
Without objection, the Chair is authorized to declare a
recess of the task force at any time.
Also, without objection, members of the full Financial
Services Committee who are not members of this task force are
authorized to participate in today's hearing, consistent with
the committee's practice.
Today's hearing is entitled, ``Perspectives on Artificial
Intelligence: Where We Are and the Next Frontier in Financial
Services.''
The Chair will now recognize himself for 5 minutes for an
opening statement.
Thank you, everyone, for joining us today at the first
hearing of the House Financial Services Committee's Task Force
on Artificial Intelligence. And I would like to begin by
thanking Chairwoman Waters and Ranking Member McHenry for
working to establish this important task force and reaffirming
this committee's commitment to understanding technological
innovation in the financial services sector.
It is an exciting time to be on this committee. Today, the
financial services sector is facing a period of rapid
disruption and innovation, and artificial intelligence (AI) is
at the heart of these changes.
AI is transforming the way Americans live, work, and
interact with each other. As members of this committee, it is
incumbent upon us to engage with and to understand more deeply
how it works, how it is designed and operated, and how it
affects and may affect consumers.
When done right, AI can mean innovative underwriting models
that allow millions more people access to credit and financial
services. And at a time when there are still over 50 million
unbanked or underbanked Americans, this is a big deal.
Companies are also using AI to execute trades, manage
portfolios, and provide personalized services to customers.
AI can be used to better detect fraud and money laundering,
and regulators are using AI to improve market surveillance and
policing of bad actors. This is important, because AI is also
giving criminals more ways to impersonate customers and steal
their assets and sensitive financial information.
Last year, there were almost 15 million victims of identity
fraud, costing Americans billions of dollars. Social security
numbers, credit card numbers, and other personal identity
factors can be stolen and sold on the dark web or used by
criminals for quick and easy profit.
That is why it is imperative that we come up with better
ways of protecting and securing our digital identities online.
In fact, I was just, in the last hour, giving a keynote speech
at the Identiverse conference, where thousands of people come
together each year to understand what technologies can be
applied to allow both individuals and organizations to protect
themselves from often AI-enabled identity fraud.
And now, as the name of this hearing suggests, the other
part of this equation that we need to explore is, where is this
technology going and what are the next frontiers?
To truly reach its potential to change the face of
financial services, there are some questions we need to
address. First, how can we be sure that AI credit underwriting
models are not biased? Second, who is accountable if AI
algorithms are just a black box that nobody can explain when it
makes a decision? And third, AI runs on an enormous amount of
data. Where does this data come from? How is it protected? Do
customers know where it is being held, under what legal regime?
Also, AI works far better with large datasets. Will these
large datasets be one more factor driving the consolidation of
financial services sectors? I worry frequently that small
community banks may end up going the way of small community
newspapers.
Another thing we will be looking at is, how many and what
kind of financial services jobs will AI displace? A recent
study by Deloitte indicated that 75 percent of financial firms
are planning to displace humans with technology, and this is
probably not a trend that will slow down. And it is not only
going to apply to bank tellers and entry-level people; it will
apply to some of the very highest salaried positions.
And as I mentioned, just the question about whether small
banks and startups will be able to compete with the big tech
firms, particularly when everyone is going to need access to
these very large, personally identifiable datasets.
Over the next 6 months, we will begin to examine these
questions to gain a deeper understanding of how this technology
is being used in the financial services industry. It is my hope
that today's dialogue between our diverse and bipartisan group
of Members and the expert panel of witnesses joining us will
lead to a better understanding of how AI is changing the
industry, how it can lead to innovative and inclusive products
and more personalized customer experience, and how this
technology will shape the questions that policymakers will have
to grapple with in the coming years.
And so at this time, I would like to recognize the ranking
member of the task force, my colleague, Mr. Hill from Arkansas,
who has been a valuable asset and a trusted bipartisan partner
as we begin this important endeavor.
Mr. Hill. I thank the chairman. I appreciate you convening
the hearing today and selecting this excellent panel before us.
And I, too, want to thank our mutual leaders, Chairwoman Waters
and Ranking Member McHenry, for their partnership in creating
this task force.
Over the next few months, I look forward to working with
you and our colleagues on both sides of the aisle to find ways
to foster innovation through the use of artificial intelligence
for both disruptive innovators and for our incumbent financial
players, both small and large, as well as finding ways to use
AI successfully to enhance our compliance obligations among our
regulatory agencies.
The use of AI has grown exponentially in the last few
years. AI has the potential to improve human life, economic
competitiveness, and societal challenges.
Recent GAO testimony identified four high-consequence
sectors where leveraging AI will bring significant benefits:
cybersecurity; automated vehicles; criminal justice; and
financial services. And today's timely hearing will discuss how
AI is impacting and influencing financial services.
Artificial intelligence can be used to gather enormous
amounts of data, detect abnormalities, and solve complex
problems. Financial institutions are already experimenting
extensively with AI strategies to enhance and streamline
financial institutions, BSA and AML compliance, CRA
requirements, fraud detection, and real estate valuations, all
while reducing cost levels.
Also, AI can create better efficiencies for underwriting
and reaching underbanked communities. Algorithmic-driven
lending is proliferating online and transforming everything
from personal loans to small business credit extension. A
recent National Bureau of Economic Research working paper found
that online financial companies discriminate 40 percent less
than loan officers who make decisions face-to-face.
I know Dr. Merrill of ZestFinance, who grew up in my
district in Arkansas, has been doing some interesting things in
regard to AI and underwriting, and I look forward to hearing
more from him today.
All that to say that the use of artificial intelligence and
machine learning is not without challenges and questions, just
like any other technology.
Dr. Henry Kissinger published an interesting article in The
Atlantic recently outlining concerns about the rise of
artificial intelligence. Dr. Kissinger argues that we are in
the midst of a technological revolution that could culminate in
a world ``relying on machines powered by data and algorithms
and ungoverned by ethical or philosophical norms.'' He goes on
to say that, ``Truth becomes relative and information threatens
to overwhelm wisdom.'' Well, we are not into overwhelming
wisdom in anything we do on Capitol Hill.
While it remains to be seen whether Dr. Kissinger's
concerns are fully proved, I think we should heed his advice.
As policymakers, we need to ensure that we are asking the right
questions about appropriate testing and evaluating of new
technology, so that the ultimate benefits are, in the end,
benefiting consumers.
We need to ensure that AI does not create biases in lending
toward discrimination and that prudential regulators and market
participants have an understanding of the underlying
technology, model validation, and how algorithmic decisions are
being made and the manner of the audit trail. These questions
must be analyzed.
Lastly, I would be remiss if I didn't mention the potential
of job losses connected with the advent of artificial
intelligence. I am sure this topic will arise throughout our
hearings during the Congress.
The World Economic Forum argues that machines and
algorithms in the workplace are expected to create 130 million
new roles in work, but cost about 75 million jobs to be
displaced by 2022, which means net 58 million jobs might be
created. In my view, this will contribute positively on the
economy and the future of work in the long run.
People might be putting the cart before the horse on the
number of net displacements. I start this journey in the ``cup
half full'' camp, and I am optimistic about our future.
I look forward to continuing to seek out answers throughout
our work on the task force. I thank my good friend, Dr. Foster,
for his partnership. And I look forward to finding bipartisan
solutions to these many interesting and challenging questions
in financial services.
I yield back.
Chairman Foster. Thank you.
Today, we welcome the testimony of Dr. Nicol Turner-Lee,
fellow at the Center for Technology Innovation, Brookings
Institution; Dr. Bonnie Buchanan, head of the School of Finance
and Accounting and full professor of finance at the Surrey
Business School, University of Surrey; Dr. Douglas Merrill,
founder and CEO of ZestFinance; and Mr. Jesse McWaters,
financial innovation lead at the World Economic Forum.
Witnesses are reminded that your oral testimony will be
limited to 5 minutes, and without objection, your written
statements will be made a part of the record.
So, Dr. Turner-Lee, you are now recognized for 5 minutes to
give an oral presentation of your testimony.
STATEMENT OF NICOL TURNER-LEE, FELLOW, CENTER FOR TECHNOLOGY
INNOVATION, BROOKINGS INSTITUTION
Ms. Turner-Lee. Thank you very much, distinguished members
of the task force, and thank you for this opportunity to speak
before you on artificial intelligence and the application of
autonomous systems in the financial services sector.
With a history of over 100 years, we at Brookings are
committed to evidence-based nonpartisan research in this area,
and my particular area of focus is on algorithmic bias. So, I
appreciate the opportunity to speak before you.
Increasingly, the public and private sectors are turning to
AI and machine-learning algorithms to automate simple and
complex decision-making processes. The mass scale digitization
of data and the emerging technologies that use them are
disrupting most economic sectors, including transportation,
retail, advertising, financial services, and energy.
These massive datasets have made it easy to derive new
insights through computers, and as a result, machine-learning
algorithms, which are step-by-step instructions that computers
follow to perform a task, have become more sophisticated and
pervasive tools for automated decision-making.
While many of us are aware of the context in which they are
used, from making recommendations about movies, to credit
products, these models make inferences from data about people
including their identities, their demographic attributes, their
preferences, and their likely future behaviors, as well as the
objects related to them. And from that data, it learns a model
which then can be applied to other people and objects, making
what they believe to be accurate predictions.
But because machines can treat similarly situated people
and objects differently, we are starting to reveal, much like
has been said, some troubling examples in which the reality of
algorithmic decision-making falls short of our expectations or
is simply wrong.
In the case of credit, we are seeing people denied credit
due to the factoring of digital composite profiles, which
include their web browsing histories, social media profiles,
and other inferential characteristics in the factoring of
credit models, and these biases are systematically finding
themselves with less favor to individuals within particular
groups where there is no relevant difference between those
groups which justifies those harms.
While my written testimony goes into more detail about
this, I would just like to share in my remaining few minutes
how we can create more fair, ethical, and just algorithmic
models. From this perspective, if we do not do such at this
time, we have the potential to replicate and amplify
stereotypes historically prescribed to people of color and
other vulnerable populations.
Let me start with an initial truth about emerging
technologies: Despite their greater facilitation of efficiency
and cognition, the online economy has not resolved the issue of
racial bias. And we see that in terms of search inquiries that
have classified African Americans as primates in the past.
These controversies are primarily due to the microtargeting
of certain populations that go awry, even when they are not
deliberate. Some of it can happen on an explicit level, where
the algorithm may not start out being discriminatory in intent
but adapts to the societal stereotypes and unfair profiling. In
the case of credit, Latanya Sweeney at Harvard University has
said that African Americans may find themselves the subject of
higher-interest credit cards and other financial products
simply because the computer has inferred their race.
In the issue of implicit or unconscious bias, we simply do
not have enough people working in this field to help us make
the right decisions, which goes back to the inclusivity and the
diversity and design of these models.
Given this--and, again, in my written testimony I speak to
the ways and the reasons of these biases, whether it is skewed
training data, whether it is the fact that we have less
counterfactual data that is actually going into training the
algorithm--these issues are nonetheless troubling and
dangerous, particularly for vulnerable populations like African
Americans and Latinos, who have been ill-served within the
financial services market. Most of these populations tend to be
unbanked compared to whites, underbanked, and lack access to
home ownership.
If you think about the physical redlining that happens
oftentimes offline, what does it mean, as Frank Pasquale has
called weblining or applications discrimination, when we begin
to look at the algorithmic economy?
What do we do about this so that we avoid unfair credit
rationing, exclusionary filtering, digital redlining? I would
just like to offer just three recommendations that I would love
to answer additional questions around that may be helpful.
First and foremost, Congress must modernize civil rights
laws and other consumer protections to safeguard protected
classes from online discrimination. We have laws like the Equal
Credit Opportunity Act, the Fair Housing Act, and other laws,
which I feel have to be modernized in the digital age to ensure
equity and fairness.
We also need companies to exercise self-regulatory
behaviors, whether it is looking at the auditing of their
algorithms, bringing in more human content moderators, or
finding ways to advance exclusivity.
And finally--and I will save this again for questions--I
think it is important that we are more deliberate in bringing
in diverse populations, partnering with Historically Black
Colleges and Universities (HBCUs) and other minority-serving
institutions, to ensure that we have more people at the table
in the design of these models.
Thank you very much, and I look forward to questions.
[The prepared statement of Dr. Turner-Lee can be found on
page 109 of the appendix.]
Chairman Foster. Thank you.
Dr. Buchanan, you are now recognized for 5 minutes to give
an oral presentation of your testimony.
STATEMENT OF BONNIE BUCHANAN, HEAD OF DEPARTMENT OF FINANCE AND
ACCOUNTING, FULL PROFESSOR OF FINANCE, SURREY BUSINESS SCHOOL,
THE UNIVERSITY OF SURREY
Ms. Buchanan. Thank you, Chairman Foster.
Distinguished members of the task force, thank you for the
opportunity to appear before you and provide testimony to help
inform discussion about artificial intelligence in the
financial services industry.
I am Dr. Bonnie Buchanan, professor of finance at the
University of Surrey Business School, and I will provide some
insights on artificial intelligence, its applications in
financial services, as well as its challenges and
opportunities. And I hope we can all work together to address
those challenges and opportunities.
Artificial intelligence is rapidly impacting the financial
services industry in a profound way, through banking,
insurance, wealth management, personal financial planning, and
regulation. It can be broadly thought of as a group of related
technologies, including machine learning and deep learning.
Machine learning deals with general pattern recognition and
universal approximation of relationships. One such example
details teaching an algorithm to learn from past regulatory
breaches and to predict new breaches, such as insider trading
or cartels.
Regulators use clustering algorithms to better understand
trades and categorize bank business models in advance of
regulatory examinations. Chatbots, powered by natural language
processing algorithms, have become powerful tools which provide
a personalized and conversational experience to users.
Deep-learning algorithms automate routine tasks, mitigate
risk, and help prevent fraud. It is based on neural networks,
which are based on mimicking the way the multiple layers of the
brain's neurons work. And neural networks have been used in
financial distress models.
Artificial intelligence offers the possibility of greater
financial inclusion, but its rapid growth and an already very
complex financial system presents major challenges regarding
regulation and policymaking, and risk management, as well as
ethical, economic, and social hurdles. For one, the financial
services workplace is going to look very different in the short
and long term, with artificial intelligence augmenting many
positions.
Machine-learning algorithms can also potentially introduce
bias and discrimination. Deep learning provides predictions,
but it does lack insight as to how the variables are being used
to reach these predictions. Hiring and credit-scoring
algorithms can exacerbate inequities due to biased data.
Policymakers need to be concerned about the explainability of
artificial intelligence models, and we should avoid black-box
modeling where humans cannot determine the underlying process
or outcomes of the machine-learning or deep-learning
algorithms.
And resolving such issues as discrimination and bias
requires being grounded in ethics and understanding what causes
the bias in the algorithm in the first place. When it comes to
artificial intelligence in financial services and a fairer
future, policymakers need to be concerned about explainability,
accountability, and, indeed, even auditability of artificial
intelligence modeling.
Many artificial intelligence techniques remain untested in
a financial crisis scenario. My written testimony discusses
several instances where algorithms implemented by financial
firms appeared to act in ways quite unforeseen by their
developers, leading to errors and flash crashes.
Cybercrime costs the global economy over $400 billion, but
many banks have started to successfully turn to artificial
intelligence techniques to address fraud through AI-based voice
phishing detection apps.
Artificial intelligence and machine learning's rapid
development are to such an extent where it is almost
outstripping the current regulatory framework. But if we look
overseas, we have in the United Kingdom the introduction of
open banking, which gives consumers the ability to compare
product offerings and exchange data between providers in a
secure way.
Under the General Data Protection Rules (GDPR), EU citizens
have the right to receive an explanation for decisions based
solely on automatic processing. Furthermore, GDPR stipulates
that companies must first obtain consent from an EU citizen
before using their data, and failure to comply with GDPR rules
can result in substantial fines.
The European Market in Financial Instruments Directive Part
II requires that firms that apply artificial intelligence and
algorithmic models have a robust development plan in place.
As big data and computing power increases, artificial
intelligence needs to be technically robust, secure, protect
privacy, and be ethically sound and regulation-compliant. We
must not forget the importance of better digital and financial
literacy, and ultimately, it needs to emphasize financial
inclusion.
Thank you very much for your time today, and I appreciate
the opportunity to share my thoughts with you later. Thank you.
[The prepared statement of Dr. Buchanan can be found on
page 34 of the appendix.]
Chairman Foster. Thank you.
Dr. Merrill, you are now recognized for 5 minutes to give
an oral presentation of your testimony.
STATEMENT OF DOUGLAS MERRILL, FOUNDER AND CEO, ZESTFINANCE
Mr. Merrill. Chairman Foster, Ranking Member Hill, and
members of the task force, thank you for the opportunity to
appear before you to discuss the use of artificial intelligence
in financial services.
My name is Douglas Merrill. I am the CEO of ZestFinance,
which I founded 10 years ago with a mission to make fair and
transparent credit available to everyone.
Lenders use our software to increase approval rates, lower
defaults, and to make their lending fairer. Before ZestFinance,
I was the chief information officer at Google. I have a Ph.D.
in artificial intelligence from Princeton University.
The use of artificial intelligence in the financial
industry is growing. Today, I will discuss a type of AI,
machine learning, also known as ML, that discovers
relationships between many variables in a dataset to make
better predictions.
Because ML-powered credit scores substantially outperform
traditional credit scores, companies will increasingly use ML
to make more accurate decisions. For example, customers using
our ML underwriting tools to predict creditworthiness have seen
a 10 percent approval rate increase for credit card
applications, a 15 percent approval rate increase for auto
loans, and a 51 percent increase in approval rates for personal
loans, each with no increase in defaults.
Overall, this is good news and should be encouraged.
Machine learning increases access to credit, especially for
low-income and minority borrowers. Regulators understand these
benefits and, in our experience, want to facilitate, not
hinder, the use of ML.
But at the same time, ML raises serious risks for
institutions and consumers. ML models are opaque and inherently
biased. Lenders put themselves, consumers, and the safety and
soundness of our entire financial system at risk if they do not
appropriately validate and monitor ML models.
Getting this mix right, enjoying ML's benefits while
employing responsible safeguards, is very difficult.
Specifically, ML models have a black-box problem. Lenders know
only that an ML algorithm made a decision, not why it made that
decision.
Without understanding why a model made a decision, bad
outcomes will occur. For example, a used car lender we work
with had two seemingly benign signals in their model. One
signal was that higher-mileage cars tend to yield higher-risk
loans. Another was that borrowers from a particular State were
slightly less risky than those from other States. Neither of
these signals raised compliance concerns.
However, our ML tools noted that, taken together, these
signals predicted a borrower to be African American and more
likely to be denied.
Without visibility into how seemingly fair signals
interact, lenders will make decisions which tend to adversely
affect minority borrowers.
There are purported to be a variety of methods for
understanding how ML models make decisions. Most don't actually
work. As explained in our white paper and a recent essay on a
technique called SHAP, both of which I have submitted for the
record, many explainability techniques are inconsistent,
inaccurate, computationally expensive, or fail to spot
discriminatory outcomes.
At ZestFinance, we have developed explainability methods
that render ML models truly transparent. As a result, we can
assess disparities in outcomes and create less discriminatory
models. This means we can identify approval rate gaps in
protected classes such as race, national origin, and gender,
and then minimize or eliminate those gaps. In this way,
ZestFinance's tools decrease disparate impacts across protected
groups and ensure that the use of machine learning-based
underwriting mitigates rather than exacerbates bias in lending.
Congress could regulate the entirety of ML in finance to
avoid bad outcomes, but it need not do so. Regulators have the
authority necessary to balance the risks and benefits of ML
underwriting.
In 2011, the Federal Reserve, the OCC, and the FDIC
published guidance on effective model risk management. ML was
not commonly in use in 2011, so the guidance does not directly
address best practices in ML model development, validation, and
monitoring.
We have recently produced a short FAQ, which we have also
submitted for the record, that suggests updates to bring the
guidance into the ML era. Congress must encourage regulators to
set high standards for ML model development, validation, and
monitoring.
We stand upon the brink of a new age of credit, an age that
is fairer and more inclusive, enabled by this new technology of
machine learning. However, ``brink'' can also imply the edge of
a cliff. Without rigorous standards for understanding why
models work, ML will surely drive us over the edge. Every day
that we wait to responsibly implement ML keeps tens of millions
of Americans out of the credit system or poorly treated by it.
Thank you so much for your time.
[The prepared statement of Dr. Merrill can be found on page
54 of the appendix.]
Chairman Foster. Thank you.
And, Mr. McWaters, you are now recognized for 5 minutes to
give an oral presentation of your testimony.
STATEMENT OF R. JESSE MCWATERS, FINANCIAL INNOVATION LEAD,
WORLD ECONOMIC FORUM
Mr. McWaters. Thank you.
Chairman Foster, Ranking Member Hill, distinguished members
of this task force, I am honored to be invited to appear before
you today to discuss this important topic.
I would like to share with you in a personal capacity key
insights from an ongoing research initiative that I lead at the
World Economic Forum. These findings are drawn from 18 months
of interviews and workshops with leading thinkers from large
financial institutions, fintech innovators, large technology
firms, and regulatory authorities, from all around the world.
It is manifestly clear that artificial intelligence is
transforming the operating models of financial institutions. It
is being deployed to improve the speed and efficiency of
financial processes, to improve the accuracy of financial
predictions, to create more accessible and personalized
advisory capabilities, and to establish entirely new business
offerings.
Less visible, but even more important, are the potential
long-term impacts of AI on the competitive dynamics of the
financial ecosystem. As AI becomes more central to the
differentiation strategies of financial institutions, their
appetite for deeper and broader datasets will increase, making
access to this data a competitive imperative for all financial
institutions.
Over time, artificial intelligence may even redraw the map
of what we consider the financial sector. For example, small
and midsized financial institutions which are unable to invest
in becoming AI leaders may instead choose to employ the AI
capabilities of third parties on an ``as a service'' basis. The
providers of these services could be large technology firms,
they could be specialized fintechs, or even competing financial
institutions.
Moreover, the tendency of AI businesses to rapidly scale
via the so-called AI ``flywheel effect'' means that successful
service providers of this kind could rapidly become central to
the operations of many financial institutions, resulting in a
deep change to the systemic structure of the financial system.
These seismic shifts in the landscape of financial services
obviously create new risks. The enormous complexity of some
advanced AI systems can make them opaque, challenging
traditional models of regulation and compliance.
The use of ever broader datasets introduces risks to user
privacy, as well as to the introduction of unintended bias into
financial decision-making. Furthermore, an inherently
specialized and interconnected financial system creates new
vectors for both the accumulation and the propagation of
systemic risk.
However, while these threats are very real and should be
taken seriously, it is critical that we avoid knee-jerk
reactions informed by fear.
In my view, the advent of AI does not call into question
the fundamental principles that inform our regulatory
framework. Rather, it demands that we be open to using both
existing and emerging techniques to ensure that we remain
aligned to these principles, even against a backdrop of rapid
technological change.
Moreover, AI's risks must be considered alongside the
opportunities that it creates. AI has the potential to help
motorists get the money that they need from an insurance claim
more quickly after an accident, to help immigrants without an
established credit history access financing, and to make high-
quality financial advice, so needed, more accessible for
everyday Americans.
Moreover, the ability to outsource selected functions to
specialized third parties has the potential to help smaller
community banks remain digitally relevant to their customers.
Ultimately, AI is a tool. As with all powerful tools,
preventing misuse is of the utmost importance. But with the
right governance and oversight, I believe that AI has the
potential to do enormous good for the financial sector.
Thank you.
[The prepared statement of Mr. McWaters can be found on
page 46 of the appendix.]
Chairman Foster. Thank you.
And I now recognize myself for 5 minutes for questions.
Dr. Turner-Lee and Dr. Merrill, the National Bureau of
Economic Research Working Paper recently published by UC
Berkeley found that the algorithmic lending models discriminate
in their case 40 percent less than face-to-face lenders for
mortgage and refinancing loans.
If that sort of result proves generally true, it is
positive news for consumers, especially African-American and
Latino consumers, who pay $765 million in additional interest
costs each year.
And it highlights the fact that the artificial intelligence
algorithms don't have to be perfect as long as they are
significantly better than the current procedures. That is
obviously a moving target, because as our underwriting gets
better and more fair over time, I think we have to continue to
ask machine-learning techniques to continually up their game as
well.
And so my question is, to what extent companies should be
required to audit these algorithms so that they don't unfairly
discriminate? Who should determine the standards for that? What
is the current understanding of best practices?
Ms. Turner-Lee. Thank you, Mr. Chairman, for that question.
I am actually also delighted to see that we are seeing
research that is actually saying that we are levering some of
the disparities when it comes to the use of AI. But I, too, am
cautious, because I think the institution of auditing practices
are really what is needed to ensure that we are not seeing
these unintended consequences of racial or ethnic bias against
different economic classes actually happening.
I would say to you that we are seeing more self-regulatory
models where companies are actually coming in and engaging in
auditing. I would also recommend, as I said earlier, that we
see developers look at how the algorithm is in compliance with
some of the nondiscrimination laws prior to the development of
the algorithm, which would also help to audit out some bias at
the onset.
A paper that we recently released also combines auditing
with a bias impact statement. There is a lot more proactive
conversation prior to the launch of the product into the public
domain.
Chairman Foster. How close are we to having generally
agreed-upon metrics for things like fairness? I remember
encountering a paper that claimed to have 15 different
definitions of fairness.
Ms. Turner-Lee. Right.
Chairman Foster. So, how do we decide which one of those is
most applicable?
Ms. Turner-Lee. That is a question with which I think all
of us on this panel today struggle. How do you look at fairness
and equity tradeoffs? Where do you find that there is a product
that is not creating more discrimination versus less? And how
do you document what those models are?
I think at this stage, our discussion around explainability
and accountability is one part of it. But I think, to your
point, getting companies as well as consumers engaged, creating
more feedback loops so that we actually go into this together,
I think is a much more proactive approach than trying to figure
out ways to clean up the mess and the chaos at the end where we
are discriminating against more people, we are incarcerating
more people, and we are denying credit to more people. We have
to figure out how to get ahead of this game.
Chairman Foster. Dr. Merrill?
Mr. Merrill. I think it is quite clear that machine-
learning models are biased. They are biased for three primary
reasons.
First, they are biased because historically, white men have
dominated the credit roles in the past, so that back data is a
bad representation of the world.
Second, they are biased because machine-learning models
tend to use a large number of signals of variables and there
has to date been relatively little best practice around, how do
you analyze those variables, because many times one or more of
them will covary to yield a protected class.
And third, they are biased because most ML models are
produced by the proverbial ``white guy in a hoodie.'' I, by the
way, own a hoodie, but I try really hard not to be biased.
I think, absolutely, we must have an audit requirement, and
I actually think a creation up-front requirement, in the way
that we today have build requirements for financial services.
FCRA produces quite striking, quite clear laws on what we are
allowed to do.
I would hope that either through congressional intervention
or regulatory intervention, we would come to a world in which
there would be a language to describe what is acceptable before
you build models and then an agreed-upon language at the end of
models to show if, in fact, you have a bias problem, because
again, the odds are good you are going to.
Chairman Foster. Mr. McWaters and Dr. Buchanan, both of you
have worked on the issue of whether or not the access to large
datasets is going to drive consolidation. Dr. Buchanan, you
have written on China, where they have simply let things
consolidate and let the access to enormous amounts of data
result in a very small number of very large players.
Are there policy options that we can do to lean against
that consolidation, in my negative 2 seconds? If you could just
say one sentence, like, read my testimony or something?
Ms. Buchanan. I do talk about this in my written testimony
and also my Turing report, Chairman Foster.
But I think we also have to understand what makes China so
different, too. Its supply of data, its online population is
twice the size of the United States. WeChat hosts over a
billion users. And they have also--
Chairman Foster. Okay. Now, I will have to; I am going to
use my power of the gavel on myself.
Ms. Buchanan. Yes, there are. We can, yes.
Chairman Foster. All right.
Now, I am happy to yield 5 minutes to Ranking Member Hill.
Mr. Hill. Thank you, Mr. Chairman.
This is a really good discussion, and I think that it is
exactly why we have this task force, to talk through these
issues.
And also, we invite our regulators to be full participants.
All of you have made that suggestion. And I think we saw
yesterday that they are eager to do that as they appoint their
own innovation officers, their own legal teams who are thinking
through this set of issues.
We are talking about innovation, we are talking about small
and large, and then we are also talking about pursuing
innovation, yet, obviously, complying with all the laws that we
have in the country. And these are doable things, right?
Nobody seeks to create a model with bias in it. In fact,
they have a legal obligation not to do that. So, there is no
group of people, hoodies or no hoodies, who are out there
seeking to generate a credit model that has bias in it.
But, Dr. Merrill, you make good points about this.
This is a problem in government, too. Let's talk about the
Consumer Financial Protection Bureau (CFPB), just a few years
ago in their settlements with Honda and Toyota, where they used
big data to estimate somebody who might have been a source of
bias in auto finance--using big data, not real customer data,
and just assumed that if your name is ``Hill'' and you are from
``72207'', you might have a chance of getting a reimbursement
from one of these settlements, based on bias. It was
fallacious, and I think this committee was stunned by that a
few years ago.
We know in government and the private sector, this is a
real challenge.
Dr. Merrill, you talked about the model development and
updating the regulatory guidance, and you have shared your
work. How do we invite those regulators to put out for a
rulemaking on updating that 2011 guidance? How would you
propose that we encourage that?
Mr. Merrill. My team who built the updates and I have spent
a long time meeting with essentially all of the regulators,
prudential and non-prudential. And one of the things that we
have found is, I think if you wandered around Silicon Valley
and asked, people would say, oh, regulators are against
innovation. And that has not been my experience at all. The
question has been, how do they do the changes in a way which
serves them well, their regulated institutions well, and
Congress well?
For me, I think the single most important element moving
forward is regulatory certainty. And I think it is impertinent
of me to suggest what Congress should do, although I am
``72032'', so--
Mr. Hill. There we go.
Mr. Merrill. --slightly different.
But even a small push to the regulators to say, we believe
ML is coming and we believe your methods of ensuring fairness,
of validating for FCRA and ECOA, and of making the promise of
ML win, would be a substantial step forward.
Mr. Hill. That is why I support the sandbox idea. I think
you all do, because you learn by doing. Of course, we are
alleging the machines are learning by doing too. So, it is a
way to backtest the reality, and I think sandboxes are useful.
We would like to see sandbox uniformity among the agencies and
a process that is open and not just--although I like the
regulatory competition. In our society, it seems to be good.
But we need to press on with that.
Also, I was comforted in a recent meeting with one of the
Federal Reserve district banks that, don't forget, we have a
lot of depository institutions that are buying credit that is
originated in this way on their books. This is a good market
test right now because we are looking at that data, we are
doing our HMDA, our fair lending analysis against those
purchase loans. And that is a way to get grassroots data as
well.
Mr. McWaters, with 18 months of research focusing around
the world on this, could you expand a little bit on why you are
in the cup-half-full camp as well on long-term employment
trends that we need? Give us some examples of these jobs that
are being created that may see roles changed.
Mr. McWaters. I can't speak to the specific methodology of
the report that you mentioned. However, I think that it is
actually quite useful when we think about this to reflect on
history. The ATM was first introduced into the financial sector
in the late 1960s, and there were some who predicted that we
would no longer have branches, we would no longer have people
in those branches.
What has happened instead is that the role that the
individuals in those branches perform is markedly different
than it was 20 or 30 years ago. It is no longer focused on
basic transaction processing, but instead focused on advice and
new sales origination.
And I think there are many examples of where we will see
the fundamental activities of a job, the things people spend
their time on, change and shift. That will likely require re-
skilling and retraining. But we won't see the job in and of
itself removed.
Mr. Hill. Thank you.
Mr. Chairman, thank you, and I yield back my time.
Chairman Foster. Thank you.
The gentleman from Illinois, Mr. Casten, is recognized for
5 minutes.
Mr. Casten. Thank you very much. And thank you so much to
Representative Foster for your leadership on this issue. It is
truly a privilege to serve on this committee. And thank you to
all the members.
I have to start with a story. I ran an energy company for a
number of years, and we had about 60 customers. Our biggest
source of budget variance every year was our inability to
predict how much energy our customers were going to use.
And, nerd that I am, I built a big genetic algorithm. We
tweaked it. And ultimately, we were able to massively cut the
revenue variance in ways that scared the pants off my
customers, because they had no idea how we did this, and
neither did I.
I mention that because what we found--I designed this to
solve for a question of how to get better accuracy in our
revenue forecast, and it did that beautifully.
The more granular I got, the more inaccurate it was. If I
asked what a specific customer was going to be, it was a little
goofier. If I asked what a specific customer's consumption of
chilled water would be, it would be goofier still. And if I
said what a specific customer's chilled water consumption was
in May, it was off the charts.
Now, we knew well enough not to use it to ask those latter
questions. But, Dr. Turner-Lee, a lot of what you described is
that we have these tools that we built to ask one set of
questions, which are really good. How do we improve our credit
evaluation? How do we improve our underwriting? But then we
have unintended consequences when we dig down to say, what does
this say about a specific individual? And I don't know how to
decouple that in the underwriting realm.
But I guess my question for you is, do you see ways,
computationally or regulatorily, to say, if we design this to
do one set of things, let's use it for that thing and be aware
to where the blind spots are, just because of the nature of the
math? These could be totally unintended. But how do we
constrain it in that way? Your thoughts?
Ms. Turner-Lee. Yes, I think that is an interesting
question. It is a regulatory question that we are looking at in
the privacy discussion right now, the extent to which consumers
give so much data that there are no start and stop points with
the accumulation of that.
I would echo what the panelists have said about the opaque
nature of algorithms. And to your point, Congressman, what we
are seeing is once it goes deeper into the ocean, the
inferences that come out of that data are what is troubling,
and are what lead to those unintended consequences.
So, we have to find ways to cure that. Do we allow
consumers to tell us when that start/stop is with regard to use
of their data? And the comment earlier about regulatory
sandboxes, do we permit for anti-bias experimentation the use
of demographic information when we know it is actually going to
help us curb bias in ways that would be detrimental to certain
populations?
I think, as you are talking about, the more granular we
get, the less accurate we are, because there are certain data
blind spots, as you suggested, that we are just not getting at.
And the way that the technology works with machine-learning
algorithms is, it assumes because a person or subject or object
has engaged in that way, that that is who they are.
And that is where we find ourselves replicating and
amplifying the stereotypes externally, because it is not the
algorithm that is saying to itself, ``I am going to be biased
today.'' It is who we are as a society and who is actually
inputting that data to create what has been considered the
``garbage-out'' variables.
Mr. Casten. The second question is for Dr. Merrill or
McWaters, you guys can arm wrestle over who gets to answer this
one.
None of you mentioned algorithmic trading. Some friends and
colleagues who are in that space have described it to me as
being: number one, awesome; and number two, completely
unhedgeable, because it is totally blind to black swan events,
because of the conversations that you mentioned. It overweights
recent data, it overweights success, and, therefore, is both
blind to black swans and, as my friend who shall remain
nameless said, potentially creates some really bizarre social
outcomes. Because if you are managing a socially responsible
fund, and all of a sudden your algorithm is trading on a bet
that we are going to invade Crimea next week, you know, weird
things happen.
How do you think we should be regulating algorithmic
trading in terms of the underlying risk, how much can we let it
penetrate the market, and what do you do with an algorithm that
is trading in a way that people may not actually understand
what the bet is?
Mr. McWaters. I think that this is an excellent point and
one that requires further investigation. We have seen in this
space a tendency for machine-to-machine interactions to lead to
feedback loops that have damaging impacts.
We have also seen that the innate foreignness that you have
referred to in terms of the way that an AI-enabled model thinks
can create confusion between fast-moving AI and slow-moving
individuals, where people effectively freeze in response to an
unexpected event. And that freezing is then interpreted as a
further negative signal by the AI, driving things to an even
more difficult situation.
Core to addressing this, in my mind, is scenario-based
modeling and the types of stress-testing approaches that we
have used in the past.
Mr. Casten. I am out of time, so I thank you.
And I yield back.
Chairman Foster. The gentleman from Ohio, Mr. Gonzalez, is
recognized for 5 minutes.
Mr. Gonzalez of Ohio. Thank you, Mr. Chairman.
And thank you, everybody, for being here.
I am really excited about the direction of this task force
and the leadership on both sides of the aisle from Dr. Foster
and my colleague French Hill, and just really excited. And
thank you for convening this.
One of my big priorities here on the committee has always
been finding ways to expand affordable credit to low- and
moderate-income borrowers. I think that has been one of the
more difficult challenges that we have faced as a society,
certainly in the financial services sector, for a very long
time.
And part of why I am excited about machine learning is
what, Dr. Merrill, you suggested, which is that we can do this.
This is something that is attainable. But there are certainly
questions.
In your testimony, you talked about how there are
``explainability models'' that aren't really doing a great job,
but at ZestFinance you have developed one or you have developed
methods that render ML models truly transparent, to directly
quote you.
My question is more on the technical side. Technically
speaking, how difficult is it to create a proper explainability
model, knowing that, from my time in tech--I used to work in
tech, not at your level--an A-plus engineer is kind of worth
about 10 midlevel engineers, if you will.
Talk to me about the technical side of this, if you would?
Mr. Merrill. Thank you for that question.
I think the way to think about it is to just kind of draw
some broad boundaries about the question at first. One of the
techniques that differs in machine learning from traditional
underwriting is you use a bunch more data, and data is
sometimes called signals.
And when you are going to do explainability, conceptually,
the hard part isn't actually comparing the inputs and the
outputs. The hard part is understanding what things inside the
models moved together to produce that output.
That essentially means you have to compare all pairs of
signals. If you have 100 signals in a model--which, by the way,
would be a very small model--you would have to compare all 100
to all other 100, which sounds easy, except that turns out to
be more computations than there are atoms in the universe,
which is a bad outcome. Well, it is a bad outcome, if you want
an answer.
The tricky part is you have to figure out how do you
optimize that in a way which guarantees correctness, but
doesn't require you to be computing until the sun burns out.
And what the mathematicians on our team have figured out a way
to do is to make those optimizations, but to do it in a way
that they can still prove the answer and we can demonstrably
answer the question of are we, in fact, accidentally
discriminating against African Americans or women.
And that is our view, is that the two things that an
explainability model must do: one, it has to successfully
optimize across the space; and two, it has to be directly
inquirable as to what do you do with respect to whatever
classes are relevant.
Mr. Gonzalez of Ohio. Thank you.
And then one thing we have talked about a lot is the data
itself. But we haven't covered as much about--Dr. Buchanan, you
mentioned it--privacy and who ultimately owns the data. I think
that is an outstanding question for sure.
And so I guess my question is for Dr. Buchanan and anybody
else who wants to take a stab at this, how should we think
about balancing the innovation that we all agree can have a
positive impact on society if we are good about it, with
protecting consumers and empowering consumers with their
individual data?
Ms. Buchanan. Thank you, Congressman.
I absolutely agree with this. And I have been very
encouraged by what I have seen in the European Union regarding
consumer protection on data and the right to own the data and
what happens with your data.
I think one thing I would like to stress to you throughout
today is, I keep hearing the term ``big data'', but I think,
moving forward, what we also need to distinguish when we are
getting down to that granular level is that big data is not the
same as strong, robust data.
Mr. Gonzalez of Ohio. Right.
Ms. Buchanan. When we are thinking about privacy, we need
to think about using strong, robust data.
And I think I would also draw your attention to my written
report where I look at China. Look at what they have been doing
with their Sesame Credit model with Ant Financial, which is not
the same as the government social credit scoring model, where
basically every data point ever collected about you goes into a
model to measure what is called ``trustworthiness.'' Not
creditworthiness, trustworthiness.
And my thoughts on this is, at the end of the day, if I am
going to look at getting a loan for a house, the data I really
want to use and protect is my loan repayment history, not my
subway fare usage, for example.
Mr. Gonzalez of Ohio. Right. Thank you.
Ms. Buchanan. And context is very important, too.
Mr. Gonzalez of Ohio. Yes, ma'am. Thank you. We will follow
up.
And I yield back.
Chairman Foster. The gentlewoman from North Carolina, Ms.
Adams, is recognized for 5 minutes.
Ms. Adams. Thank you, Mr. Chairman.
First of all, let me, before I begin my questions, I want
to thank you for the opportunity to serve on this task force.
And I am looking forward to it, along with you, and my friend,
Congressman Hill.
To the witnesses today, thank you so much for your
testimony.
As technology becomes more and more commonplace, it is
critical that we proactively address issues that could
positively and negatively impact our constituents and our
financial institutions.
Algorithms have become a part of everyday life, even though
most Americans have limited awareness or understanding of these
systems and their impact. Increasingly, public and private
enterprises have turned to artificial intelligence software and
machine-learning programs to help increase the effectiveness of
the services rendered.
Let me begin by addressing this question to Dr. Turner-Lee.
There have been concerns about bias in AI systems, such as the
potential of historical biases in datasets to be perpetuated or
amplified in AI systems. How do firms ensure that AI systems
are not having a disparate impact on vulnerable communities?
And what safeguards should regulators and Congress put in place
to protect consumers?
Ms. Turner-Lee. Thank you, Congresswoman, and thank you for
that question.
I am going to just give three points that I think need to
be injected into this debate.
One is diversity in the workforce. The developers who sit
at the table in the design of algorithms are not representative
of the colorful spectrum of people who actually are using these
algorithms. And, as a result, I think that we miss
opportunities to have a seat at the table to mitigate issues
related to gender or race or even background. I am a
sociologist sitting among computer scientists. We need more
perspectives with regards to that.
And I think to push for inclusion, we also need diversity
in design. We wrote a paper at Brookings that is really about
sitting at the table and thinking through what may become the
intended and unintended consequences of these models. How are
they replicating stereotypes that we see? In what ways should
companies be trying to put in best practices that avert those
types of discriminatory actions?
People of color, in particular, have not come this far to
have technology become one of the major elements of further
discrimination and amplified bias. And so, we have to be
proactive in increasing the number of data scientists who are
engaged in this, who come from diverse backgrounds, and also
creating, I think, a standard, particularly in the sensitive
use cases like financial services, employment, and housing,
where people of color have already been historically
disadvantaged, that we have to ensure that these sensitive use
cases are not open for business with regards to doing further
damage.
Ms. Adams. Great. Thank you.
Dr. Buchanan, within the context of financial services,
have you seen the potential for bias in the use of AI? And how
are various countries handling this issue? What should
policymakers do to ensure the use of AI doesn't discriminate
against vulnerable communities?
Ms. Buchanan. Some of the more notable examples that I
highlight in my report, Congresswoman, relate to how algorithms
are used in the peer-to-peer lending industry. And so, just to
follow on from Dr. Turner-Lee's comments, I can refer you to a
paper where I found that peer-to-peer listings where African
Americans provide their pictures on the lending site are
roughly 3 percent less likely to be funded and receive a loan
and are more likely to pay higher basis points than white
people with similar credit profiles. The examples I detailed in
my reports are particularly pertinent in the debt
consolidation.
Ms. Adams. Okay. Let me ask a yes-or-no question: Would it
be useful for Congress to fund algorithmic bias research
through NSF, NIST, and other Federal agencies, to develop
tools, methods, and programs to resolve bias in artificial
intelligence systems? If I can get a yes or no?
Ms. Buchanan. Absolutely, yes.
Ms. Adams. Okay. Dr. Turner-Lee?
Ms. Turner-Lee. Yes.
Ms. Adams. Dr. Merrill?
Mr. Merrill. Yes.
Ms. Adams. Mr. McWaters?
Mr. McWaters. Yes.
Ms. Adams. Okay, very good. Thank you very much.
Dr. Merrill--and I know we don't have a lot of time--what
steps should companies and policymakers take to address this
concern? Can you give me one?
Mr. Merrill. I think the most important thing that
regulators and policymakers should do is provide clarity. Even
clarity that is not perfect is better than uncertainty to get
companies to innovate in a good way.
Ms. Adams. Great.
Thank you, Mr. Chairman. I yield back.
Chairman Foster. Thank you.
The gentleman from North Carolina, Mr. Budd, is recognized
for 5 minutes.
Mr. Budd. Thank you, Chairman Foster. I want to commend you
and my friend Ranking Member Hill for all your work on this
task force.
I am excited that you all are here today.
And I want to start my time by highlighting the potential
impact that machine learning and AI can have in our insurance
market for institutions and their customers. But before I do
so, I want to ask permission, Mr. Chairman, to enter into the
record this report from the GAO. It is entitled, ``Insurance
Markets: Benefits and Challenges Presented by Innovative Uses
of Technology.''
Chairman Foster. Without objection, it is so ordered.
Mr. Budd. Thank you, Mr. Chairman.
This report highlights how AI and machine learning benefit
insurance markets and the consumer. I am excited to explore how
this technology can improve underwriting accuracy, facilitate
stronger communication with customers, make the claims
processes easier to navigate for the consumer, and combat
insurance fraud, among many other things.
Let me just highlight one specific provision from the GAO
report, that is found on page 11. The report highlights
telematics, which is the combination of telecommunications and
information processing to send, receive, and store information
related to specific items such as automobiles and water
heaters. And I happen to have one of those water heaters, and
it never knows when the in-laws are coming and when all the
kids are home from college.
Telematics allows sensors in an automobile to provide data
on a driver's behavior such as speed, hard braking, and turning
radius. Now, according to the GAO report, insurers can then use
that information to determine the driver's risk profile and
help determine the premium rate for that driver, if a driver so
chooses.
So, I encourage my colleagues to read this report that was
requested by Ranking Member McHenry as we move forward with
this task force with any potential policy proposals. Thank you.
I am sure we all agree that the U.S. must stay at the
forefront of this new technology in the financial sector, like
artificial intelligence and machine learning.
And here is the question. It is for Mr. McWaters: What
challenges are companies facing that inhibit them from
achieving the full potential of these emerging technologies?
How are overly burdensome regulations stunting growth in this
area? And how can our committee ensure that proper controls are
in place to protect customers while also fostering growth in
AI?
Mr. McWaters. Thank you very much.
I think that one of the most significant instances of where
we see challenges to responding to this on the part of
particularly incumbent financial institutions are the legacy IT
systems that are in place.
Typically, data is heavily siloed, making it difficult for
that data to be ingested and used by conventional machine-
learning methods, and the systems themselves, while extremely
robust and resilient, are not as adaptable as modern and
particularly cloud-based computing methodologies.
Interestingly, one of the things that we have seen in this
space--and this pertains to some degree to Chairman Foster's
question about consolidation--is that there is an opportunity
for third-party service providers to play a helpful role in
enabling financial institutions to leapfrog forward, in terms
of their capabilities.
By plugging into specialized fintech or regtech firms, into
large tech firms which might offer, for example, machine vision
as a service, you might as an insurance entity be able to use
that machine vision to accelerate the processing of minor
automotive claims, for example.
I think that, in terms of the discussions that I have
internationally, one of the perceptions of the United States in
this space is that the regulatory environment is extremely
complex to navigate and that the large number of regulatory
entities creates challenges to deploying new innovations
effectively.
I don't have a specific remedy for that, but it certainly
is one of the contributors to the challenge of deploying these
technologies here in the United States.
Mr. Budd. I appreciate that, Mr. McWaters. And continuing
on with you, besides lower cost of financial products and
services, what are some other ways in which a consumer stands
to benefit from adoption of these technologies in the financial
services?
Mr. McWaters. I think one of the particular items here is
the opportunity to provide valuable advice and intervention for
clients. So, if you pursue the example of insurers that you
gave, telematics has an opportunity to, on one hand, support
more accurate and more personalized underwriting, but it also
increasingly has the potential to give drivers valuable
feedback on how they might be safer drivers.
The water heater that you mentioned might be able to alert
you if there was a leak, allowing you to minimize the damage to
your home in a way that is beneficial both to you and to the
insurer who has provided that cover.
Mr. Budd. It sounds like a lot of opportunities.
With that, I yield back. Thank you.
Chairman Foster. Thank you.
And after consultation with the ranking member, I would
like to inform Members that we are going to have time for a
second round of questions, subject to the fact that we have to
be done here by 11:30. So, we should at least have a partial
second round here.
I now recognize the gentlewoman from Texas, Ms. Garcia, for
5 minutes.
Ms. Garcia of Texas. Thank you, Mr. Chairman, and thank you
for having this hearing. And I thank Chairwoman Waters for
really focusing on this issue, because it is so important as we
move forward.
However, I think it is one that is kind of confused, and I
wanted to just start with a question. I was trying to figure
out which professor to ask, so I am going to go ahead and go
with a woman. I, too, have some biases.
Dr. Buchanan, for those who are watching who are not in the
financial industry, who don't know what artificial intelligence
means, they hear the word, ``intelligence'', and they think it
is some really super big-brother secret stuff. Can you in just
plain English, in 25 words or less, tell the average viewer
what the heck we are talking about?
Ms. Buchanan. First of all, there is no generally agreed
upon definition of ``artificial intelligence.''
Ms. Garcia of Texas. You are using up your 25 words now.
You are talking straight to the average consumer in the United
States.
Ms. Buchanan. Okay. I would say it is a group of
technologies and processes that can look at determining general
pattern recognition, universal approximation of relationships,
and trying to detect patterns from noisy data or sensory
perception.
Ms. Garcia of Texas. I think that probably confused them
more.
Ms. Buchanan. Sorry.
Ms. Garcia of Texas. With all due respect, but I think that
is one of the challenges that we have. I wanted to do that, not
to make light, but just to accentuate the problem that we are
facing, because I think there is an idea that now all these
robots are going to take over all the jobs and everybody is
going to get into our information, this whole balance that one
of my colleagues mentioned between privacy and the markets. So,
I think it is important.
Ms. Turner-Lee, one of the things that would help us better
understand it, I think, are some of the things you pointed out,
in terms of diversity of the people at the table who are
developing the software, the people who are the workforce
involved.
If you could name the single one thing that Congress could
do, I mean, we can't change attitudes. We probably can't change
some of the criteria that the folks who are putting this
together are looking at. What would you suggest that one thing
be?
Ms. Turner-Lee. Yes. That is such an interesting question,
because I think the tech diversity issue has been one that
Congress, as well as civil society actors and others, have
really grappled with. And as we see technology evolve in the
way that it is to a point where it is confusing, I would
suggest that we have a lot more to do as these become much more
ubiquitous and widespread.
On your question, I think what Congress can do first to
quell algorithmic bias is to create guardrails. I think it has
been mentioned that we need to ensure the tech companies know
that they have to be in compliance with antidiscrimination
laws. I think we start there. We create guardrails for best
practices in design and development.
With regards to creating more diversity at the table, these
are companies that are not necessarily regulated or in any way
required to report diversity, in terms of who they serve and
who is sitting there. But I think we should reward best
practices where we are seeing demonstrations of companies
wanting to bring more actors to the table.
What does that mean? Years ago, when we had the ENERGY STAR
standard imposed on appliances, most of us who go into a big
box store know this appliance is going to save us money and it
is going to be safe.
I think we should push in the algorithmic economy a gold
standard: What is the Energy Star rating for what consumers
understand of how their data is being used? And how will
companies pushing the bar, raising the expectation that they
are going to be in compliance, not only with those
nondiscrimination laws, but they are going to be good stewards
of our information and they are going to have environments
where diversity is encouraged?
Ms. Garcia of Texas. Is there anything that we can do in
terms of the criteria that they are using? Because I know one
of the examples you gave on gender bias was just the word
``woman'' being on their resume somewhere caused to trigger the
gender bias.
What can we do with regard to the criteria being used? For
example, if you looked at my resume, I graduated from a
Historically Black College, and I would hope that there is no
assumption that I am African American, but a computer could do
that, right?
Ms. Turner-Lee. That is right.
Ms. Garcia of Texas. But I also go to a women's college,
so, obviously, that is going to peg me in that. But then they
look at me, and I don't look like I am Latina.
Ms. Turner-Lee. That is right.
Ms. Garcia of Texas. I am going to have one confused
computer.
Ms. Turner-Lee. That is right. And you are going to have a
double or triple jeopardy, right?
Ms. Garcia of Texas. But is there any way that we can do
anything about what gets in the computer?
Ms. Turner-Lee. Yes, as a policymaker myself at Brookings,
it is so challenging to figure out how do we get companies to
sort of adhere to a standard without overregulating them? And
that is why I think those guardrails are particularly
important.
But I also think it is important for us to continue this
discussion on what does disparate impact mean when collective
groups of people are denied loans or denied credit or denied
some form of equitable opportunity in this country simply
because the computer was wrong. Who is liable for that? Is it
the developer?
I actually agree with what was said earlier. I don't think
developers necessarily walk around in a hoodie saying, ``Today,
I am going to discriminate against people.'' I think it is the
nature of what is in the black box that is not understood,
which is why explainability models matter.
People need to understand what is going into this ocean.
And for the layperson, I will give you this example that I use.
It is like swimming in the ocean. At the top, you can see my
legs and my hands, but when you go down, you begin to not see
my body because the water becomes really cloudy.
I am okay if I actually search for camping gear for my son
on one site and it shows up on another site. I am not okay if I
am profiled because I am an African-American woman or a woman
who went to a Historically Black College, et cetera. Those are
things that I can't see how you even got there to understand
that from just my hand sticking out.
And so, we have to figure out what are those guardrails
that will protect people, where are there pressure points to
institute some other consumer protection, what is the role of
privacy in terms of the data that is collected on people?
And I would suggest to you, where in the process can I
recurate my identity and let them know that, ``Hey, I am not
this person that you keep thinking I am just because I buy
camping gear. It is not me going out; it is my son.''
Ms. Garcia of Texas. It is a good point. Thank you.
Chairman Foster. Thank you.
Ms. Garcia of Texas. I yield back. Thank you, Mr. Chairman.
Chairman Foster. This is a wonderful discussion that could
go on forever.
The gentleman from Virginia, Mr. Riggleman, is recognized
for 5 minutes.
Mr. Riggleman. Thank you, Mr. Chairman, and thank you to
Ranking Member Hill, and thank you to all of the witnesses for
being here.
I would like to start by saying I am proud to be a member
of the inaugural Artificial Intelligence Task Force. And I was
going to send my avatar today, but it kept going in circles and
bumping into walls, so I said, I am going to come here myself.
That was a bad, bad joke.
But, anyway, my background experience with data analytics
has taught me a lot, especially about the evolution I
personally witnessed since 2002. And to get to my questions, I
just want to talk really quickly about what I have done. My
experience might be a little bit different than everybody up
here.
I have been trying to aggregate big data and analyze big
data for predictive analysis to go after actually network
centers of gravity and critical touchpoints for a long time in
the nonkinetic space on the military side.
And back in 2002, I want to tell you guys, that the big
thing about the military--we have this incredible saying, that
we try to solve today's problems with yesterday's technology
tomorrow.
I think what I saw in 2002, there was never a statement of
AI or machine learning. We were using these just really kludgy
relational databases, trying to build arbitrary translators to
try to make sure the nodes and attributes actually made sense
for unproductized data, productized data, but mostly data that
just didn't make a lot of sense to us in 2002.
What we have seen in the last 5 years, and I know this is
crazy because sometimes the DOD is a little bit behind, but it
is our work with places like Johns Hopkins University's
Federally Funded Research and Development Centers (FFRDCs),
working with the physics labs. And now you see a lot of not
only private-public partnerships, but you see a lot of
commercial and government partnerships in big data.
And what we have seen going forward is, that 5 years ago we
might have been using relational databases, but now we are
using graph databases and dynamic translators we could have
never foreseen in the future. We had about 40 people working
with us trying to find every touchpoint and every critical node
in a network. So, I went from dropping bombs to actually
dropping nonkinetic bombs, right, in specific types of
networks, is pretty much what we did.
And it is just amazing to me, listening to all of you, that
my background is so different, just based on trying to work
with data, and the fact that machine learning and artificial
intelligence, even up until 2010, 2011, in the military space,
and big data with my companies, we really didn't talk about it
much. We just really didn't. But now we can.
And what we see now is that now we are getting
unproductized data. We are getting disparate data, multiple
datasets. I am getting natural language processing. We are
getting tons of unstructured data. We are able to go into
dynamic translators we can put into graph databases, and now we
are actually coding to what people are thinking when they are
looking at a specific problem set. We are coding to an
analyst's brain serially in parallel. Now, we have machine-
learning templates.
And here is what happened after all that incredible stuff:
It failed miserably the first time, because we were missing so
much data.
The thing that I am going to ask, because I have my own
reasons about this, and I will ask Mr. McWaters first, when you
look at AI and ML, when you are looking at ML templates,
machine-learning templates, when you are looking at what
artificial intelligence is, the difference between templating
and the difference between rules, where do you think the split
is? And I want to ask some of you, where do you think the split
is because definitions of machine learning and AI?
I know I have my own, but I would love to hear from you,
because sometimes I even get sort of wrapped around the axle in
trying to figure out where that split is and where we can
actually look at some of the safeguards to make sure that we
make the right jump from ML to AI.
Mr. McWaters. There is an old joke that artificial
intelligence is whatever a computer can't do yet.
Popularly, our definitions of this have tended to move over
time. Twenty years ago, you might have said that a computer
would be intelligent if it could beat a grandmaster at chess.
Today, we sort of think of that as being a relatively trivial
case of intelligence. We think of it as being programmatic.
So, I think our definition of artificial intelligence tends
to move over time. And, as Dr. Buchanan said, I don't think
there is a clear articulation of exactly which techniques--ML,
deep learning, and others--are specifically rested under the
umbrella of that definition.
Mr. Riggleman. Dr. Merrill?
Mr. Merrill. I think we can spend a lot of time trying to
get our heads around the different definitions. When I started
in the field, which is a long time ago now, AI was generally
thought to be machines that tried to actually reason, that
tried to start with an initial point and take steps to get to
an end point, whereas ML was viewed more as just rote math,
just like throw a computation at the problem.
Mr. Riggleman. Right.
Mr. Merrill. You can still sort of throw that distinction
out, but it just turns out to be a little bit unhelpful at the
end, because AI failed when I started and it is roughly still
failing, because it is just a really hard problem. People turn
out to be really, really complicated beings.
And stuff which we said could never get done until AI
worked is now relatively trivial in ML. To wit, your car's
brakes are better than you are. And that is a case of ML that
we said could never be done. You could never compute friction,
but it turns out you can.
Ultimately, I think the most important class is maybe not
whether it is AI or ML, but rather what are the characteristics
of the problem you are trying to solve? AI-based techniques are
trivial to explain. ML techniques are quite a bit harder to
explain, but quite a bit more powerful. And so I guess I would
encourage us to think less about the technique and more about
the category of problem.
Mr. Riggleman. Thank you.
And that is why I am so excited about this. Thank you, Mr.
Chairman. Because I think we have a chance to really solve some
problems here, and I am happy to be here. Thank you, sir.
Chairman Foster. Thank you.
The gentleman from Georgia, Mr. Loudermilk, is recognized
for 5 minutes.
Mr. Loudermilk. Thank you, Mr. Chairman.
I appreciate the panel being here. It is a very intriguing
discussion we are having here today, especially as I spent 30
years in the information technology industry, as my good
colleague, Mr. Riggleman, also spent time in the intelligence
community in the Air Force in the earlier days where we were
using analytics of massive amounts of data. And what is
happening in that arena today is light years beyond anything
that we were able to do with rooms full of main processing
systems, mainframes back in the time.
And I am really interested in this field today, in what we
can do with our artificial intelligence. I think it is also as
important to understand our limitations of what we can't do and
draw our boundaries around that, but yet on the periphery of
that boundary having the sandboxes to where we can test and we
can implement what we may be able to do in the future once we
stabilize that.
One of the things I am interested in is what can we do
today with artificial intelligence and fraud detection and
prevention, because that is something that is really important
in the industry, especially as we move more in the fintech
arena.
My line goes back to the chip card industry. Since I have
been in Congress, when I first started here, my debit card and
my credit card had a chip, but I could only use it when I
traveled overseas.
Once we implemented that ability here, the fraud went down
by 76 percent. But criminals being criminals, all they do is
shift their focus, and that focus has gone over into the
digital payments arena, which is where we have a lot of
challenges today.
And, Dr. Buchanan, I appreciate your discussion that you
brought up in your testimony about how one of the payment card
networks is using AI to help financial institutions reduce
their fraud by $25 billion annually. Can you tell us more
detail about how payment processors-- financial institutions,
insurance, retail, and others are using AI to combat the
digital payment fraud?
Ms. Buchanan. When we are thinking about AI's automating
simple and complex decisions--actually, that is my 10-word
definition, so I think I have redeemed myself, Congressman.
One area that I can address to you is that 50 percent of
phishing detections are now finance-related. And so what I
detail in my report are some very encouraging examples around
the world where financial services companies have tried to
reduce phishing attacks.
There is a really good example in my report, IBK, a
phishing voice detection app, and it is really a coordinated
effort between regulators in South Korea and the financial
services industry.
Basically what this app looks at is--and phishing in South
Korea accounts for millions of dollars a year--a phone call is
made, and it looks at picking particular keywords in the phone
call. And if it meets a particular threshold, then an alert
signal is sent that this is a potential voice phishing scam,
and a significant financial transaction is halted.
In Estonia, Monese is using artificial intelligence in this
arena as well, particularly when they are trying to on-board
customers in the first place. So, they are looking at matching
documents with video selfies in order to detect fraudulent IDs
and fight identity theft.
Mr. Loudermilk. I traveled to Estonia last year, and what
they are doing in the fintech industry is really a model for a
lot of other nations. It is surprising, especially being an
Eastern Bloc country, the suppression that they had during
communism, to be able to come out to where they are now.
Regarding the things you just explained to us, payments.com
showed that less than half of financial institutions use AI for
fraud prevention. Why are we not seeing more use in the
industry for fraud prevention?
Ms. Buchanan. That is an interesting question, Congressman.
I think really it is because detecting fraud in the first
place, we think about fraud as really being a latent variable.
I mean, it is not necessarily directly observable, and so it is
more challenging to machine-learning algorithms.
Actually, in some sense, you have a little bit of a self-
defeating goal here. You could have the case of falsely
declining transactions as fraudulent, okay. That actually costs
the industry a lot in lost customer loyalty each year.
And apart from this erosion of customer loyalty and loss of
retail losses, the machine-learning algorithms to detect fraud,
as I said, they are more latent, in the sense that it is easier
to track someone's shopping history directly. You see what they
purchase. You see what they buy. But fraud is just another
layer. It is not as directly observable. And I think that
presents a complexity to the process.
Mr. Loudermilk. Thank you.
Chairman Foster. Given the time constraints on our
occupancy of this hearing room, it looks like we will have time
for only 5 minutes of questioning by the ranking member and the
Chair. So, I would now like to recognize the distinguished
ranking member for 5 additional minutes of questions.
Mr. Hill. I thank the chairman.
I thank, again, the panel for being here today. I
appreciate your contributions to this important beginning of
the task force work for this Congress.
Mr. McWaters, I wanted to start with you and just talk
about some of the ways today that you are seeing AI being used
in the financial services industry.
So, if you would talk about two or three of the biggest
ways you are seeing artificial intelligence being used by the
financial industry in customer acquisition, extension of
credit, regulatory compliance costs? Name two or three or four
specific elements in each of the main areas, if you would.
Mr. McWaters. I think we are seeing four key ways in which
this is being deployed in financial services.
The first is driving increased efficiency, being able to do
the same thing faster and with less manual input. And that can
be a benefit both to the organization, obviously, in their
bottom line, but also to the consumer, who is able to get an
answer to their question or to their request more quickly.
Second, we are seeing an improvement in outcomes. Dr.
Merrill made reference to this in terms of being able to
originate more loans, accept more applications without a
significant increase in defaults.
Third, we are seeing entities build out entirely new
businesses. By virtue of some data flow that exists, is
propagating through already, you may be able to create new
value propositions. So, a payment network might be able to
create a business of macroeconomic forecasting based on the
data that flows through their network and monetize that
separately.
And then finally, advice. Americans struggle to access the
financial advice that they need to make good financial choices
in the moment to plan for retirement. That advice traditionally
has needed to be delivered by expert individuals and can be
very expensive.
We are at the very beginning, I believe, of the opportunity
to provide high-quality advice to individuals in real time that
will help to address that issue. It is nascent today, but the
opportunity is quite significant.
Mr. Hill. On that point, I believe in making sure that we
have an economy that offers choices to consumers from the whole
spectrum of the most machine-led robo-adviser to the most
sophisticated one-on-one consultation. I don't think that
government policy should bias towards that, and we have had
some debates over the last 4 years where I think government
policy actually directed people away from advice to machine-
driven robo-advisers.
If I go through a sharp downturn in my portfolio and it has
been dependent on a robo-adviser, who am I holding responsible
for that? Who can I go talk to about that?
Mr. McWaters. I think that is an open question.
Mr. Hill. I don't like open questions. That is why we are
here today. We need to make sure that those consumers know the
risks of that. And that may be the trend of the moment or the
trend of the time or it may be, in the short run, more
affordable, but those are the kinds of things I think we have
to talk about here in this, in our work.
Mr. McWaters. I would also note that I think that you will
see in this space that even amongst some of the sort of highest
echelons of private banking, what we now see is an appetite by
those consumers to have a mix of both automated and in-person
mediated items.
The other thing that I would note in response to your
earlier question about consolidation in the marketplace is that
these technologies can also provide an interesting opportunity
for small and midsized financial institutions to rapidly catch
up to large entities.
Mr. Hill. I do share your optimism there. All through the
technology cycle, going back from a mainframe to a business
size computer to the cloud, small broker-dealer competitors and
small financial services competitors have had access to scaled-
up technology through a vendor platform that in some ways helps
them do a better job of being in full compliance of risk.
Data privacy, if each of you would just quickly answer, do
you support the use of APIs when it comes to protecting
customer service, customer data interfaces between aggregators
or individual companies?
Dr. Turner-Lee, do you want to start?
Ms. Turner-Lee. Yes, I do.
Mr. Hill. Dr. Buchanan?
Ms. Buchanan. Yes, I do.
Mr. Hill. Dr. Merrill?
Mr. Merrill. Yes, I do.
Mr. Hill. Mr. McWaters?
Mr. McWaters. Yes, I do.
Mr. Hill. Good. Thank you. I yield back.
Chairman Foster. Thank you. And I guess as a follow-up on
the API question, what do you think the state of the art is for
authenticating yourself for access to those APIs?
Because one of the scariest things that I see about
artificial intelligence is just the very impressive high-
quality tools being used for phishing. Things, for example,
where they will listen to your voicemail response, use that to
synthesize your voice, and fake a phone call to one of your
friends in your contact list saying, ``Hey, Joe, I just sent
you an email with an attachment, can you have a look at the
attachment and call me back?'' And everyone clicks on that
attachment. And that is not even mentioning the video that is
now available.
I think one very valuable thing the government can do is to
at least provide citizens who are interested in having a high-
quality way of digitally authenticating themself online very
much in the way Estonia has been leading the way.
And my closing question, I guess to each of you is, we have
about 1 minute for each, if you look forward at the competitive
environment, you see all of the giant banks trying to--they all
have 10-year plans to turn themselves into tech firms. All of
the tech firms are getting into banking as rapidly as you can
imagine.
And so looking forward a decade, what do you think about
the competitive landscape? Will there be any difference between
giant financial institutions and tech firms, as we know them
now?
Just march down the line.
Dr. Turner?
Ms. Turner-Lee. I think we are going to go in this era of
converged services, and it is going to be very challenging for
regulators and Congress to discern what guardrails apply to
whom. And right now, we have strong sectoral policies that
affect the financial services sector, and we have loosely
regulated policies that may apply to tech companies.
I think going forward we are going to have to figure out,
particularly on behalf of consumers, where do those protections
lie and where do we again place pressure for regulatory
frameworks that allow for innovation while at the same time
putting some stresses around the fact that we cannot have
permissionless forgiveness in areas that have huge consequence
for consumers.
And so, I completely agree with you. I think at some point,
the lines are going to be so blurred we are not even going to
know.
But keep in mind it has been consumers who are driving that
demand for these services. So, I agree with you as well, we
have to do--
Chairman Foster. And in Congress it is, obviously, a big
issue, because I think there are seven committees that claim
they are doing some part of IT, information technology, which
means, of course, no one is doing it.
So, Dr. Buchanan, any thoughts on this?
Ms. Buchanan. The landscape I see moving forward, Chairman
Foster, is more mergers and partnerships between banks,
financial institutions, and big tech companies.
I do agree with Dr. Turner-Lee about drawing this line
about how data is used. And I am very concerned, moving
forward, that I want to make sure we don't give up privacy at
the expense of convenience.
Chairman Foster. Thank you.
Dr. Merrill? And also, if you could comment on the role of
the startup in this, where they may or may not have access to
these giant datasets that seem to be essential for success in
AI?
Mr. Merrill. I guess I will be a little bit of an outlier
here amongst my distinguished colleagues.
I think there is essentially no chance that in a decade we
will see mergers and material consolidation between technology
companies and big banks, because the cultural differences will
be so great that the mergers will blow up.
I was responsible for a variety of our financial products
when I was still at Google, all of which were carefully
regulated really, because we were a bit weird about that. And
it was clear that that was the wrong place to do those, those
products, not because anyone had the wrong intent, but just
because it just didn't fit.
I think ultimately, startups are at material risk, and I
think that is very dangerous for the U.S. economy. We are at
risk because it is hard to get data. We are at risk because a
brief sideswipe by a large company, let alone the government,
will crush any of us.
And I think over the last 20 years, for good or for ill, we
have seen a lot of the development in this economy coming from
startups. So, my biggest worry is that.
Chairman Foster. Mr. McWaters?
Mr. McWaters. I would argue that we need to think outside
the bank, if you will, that we think about financial services
in a heavily verticalized and siloed fashion. We need to think
about it in a more modular way.
And so when I look forward to the 10-year landscape, I
would predict a world in which customer experiences for
financial services increasingly trend towards the best of what
big tech can offer, whether that is offered by a traditional
financial entity or a technology entity, but that the products
that the consumer accesses, the loans, the insurance, they need
to fundamentally remain regulated.
And the data that is used to inform the entire experience
needs to become more secure, the customer needs to have more
control, and we need to really enfranchise the customer within
a regulated framework.
Chairman Foster. Thank you.
And I would like to thank all of our witnesses for their
testimony today.
The Chair notes that some Members may have additional
questions for this panel, which they may wish to submit in
writing. Without objection, the hearing record will remain open
for 5 legislative days for Members to submit written questions
to these witnesses and to place their responses in the record.
Also, without objection, Members will have 5 legislative days
to submit extraneous materials to the Chair for inclusion in
the record.
This hearing is hereby adjourned.
[Whereupon, at 11:36 a.m., the hearing was adjourned.]
A P P E N D I X
June 26, 2019
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]